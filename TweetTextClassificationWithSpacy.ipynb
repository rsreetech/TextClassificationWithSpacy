{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let us look at how we can perfom text classification with spaCy\n",
    "The dataset is from the Tweet Sentiment Extraction challege from Kaggle(https://www.kaggle.com/c/tweet-sentiment-extraction/overview)\n",
    "We would perfom text classification using spaCy on tweet data to classify tweets as \"positive\",\"negative\"  or \"neutral\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from spacy.util import minibatch, compounding\n",
    "import sys\n",
    "from spacy import displacys\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define methods to pre-process the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our training data to spaCy format to train the categorization model. \n",
    "<br>\n",
    "The train data format looks like:\n",
    "<br>\n",
    "[\n",
    "{'some text', {'cats': {'Class_Label1': 1, 'Class_Label2': 0, ...'Class_Labeln': 0},\n",
    "{'some text', {'cats': {'Class_Label1': 0, 'Class_Label2': 1, ...'Class_Labeln': 0},\n",
    "{ ...}]\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_spacy(file_path):\n",
    "  \n",
    "    train_data = pd.read_csv(file_path)\n",
    "    train_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "    train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split())) \n",
    "    mask = train_data['Num_words_text'] >2\n",
    "    train_data = train_data[mask]\n",
    "    print(train_data['sentiment'].value_counts())\n",
    "    \n",
    "    train_data['text'] = train_data['text'].apply(remove_emoji)\n",
    "    train_data['text'] = train_data['text'].apply(remove_url)\n",
    "    train_data['text'] = train_data['text'].apply(clean_text)\n",
    "   \n",
    "    train_texts = train_data['text'].tolist()\n",
    "    train_cats = train_data['sentiment'].tolist()\n",
    "    final_train_cats=[]\n",
    "    for cat in train_cats:\n",
    "        cat_list = {}\n",
    "        if cat == 'positive':\n",
    "            cat_list['positive'] =  1\n",
    "            cat_list['negative'] =  0\n",
    "            cat_list['neutral'] =  0\n",
    "        elif cat == 'negative':\n",
    "            cat_list['positive'] =  0\n",
    "            cat_list['negative'] =  1\n",
    "            cat_list['neutral'] =  0\n",
    "        else:\n",
    "            cat_list['positive'] =  0\n",
    "            cat_list['negative'] =  0\n",
    "            cat_list['neutral'] =  1\n",
    "        final_train_cats.append(cat_list)\n",
    "    \n",
    "    training_data = list(zip(train_texts, [{\"cats\": cats} for cats in final_train_cats]))\n",
    "    return training_data,train_texts,train_cats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert our train data and test data to spaCy format"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data,train_texts,train_cats   = load_data_spacy(\"C:\\\\TweetSenitment\\\\train.csv\")\n",
    "print(training_data[:10])\n",
    "print(len(training_data))\n",
    "test_data,test_texts,test_cats   = load_data_spacy(\"C:\\\\TweetSenitment\\\\test.csv\")\n",
    "print(len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a method to evaluate our text categorization model. I use classification_report from sci-kit learn to get the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort(sub_li): \n",
    "  \n",
    "    # reverse = True (Soresulting_list = list(first_list)rts in Descending  order) \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used \n",
    "    return(sorted(sub_li, key = lambda x: x[1],reverse=True))  \n",
    "\n",
    "# run the predictions on each sentence in the evaluation  dataset, and return the metrics\n",
    "def evaluate(tokenizer, textcat, test_texts, test_cats ):\n",
    "    docs = (tokenizer(text) for text in test_texts)\n",
    "    preds = []\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        #print(doc.cats.items())\n",
    "        scores = Sort(doc.cats.items())\n",
    "        #print(scores)\n",
    "        catList=[]\n",
    "        for score in scores:\n",
    "            catList.append(score[0])\n",
    "        preds.append(catList[0])\n",
    "        \n",
    "    labels = ['positive', 'negative','neutral']\n",
    "    \n",
    "    print(classification_report(test_cats,preds,labels=labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now let us train a text classification/categorization model in spaCy for classifying sentiment of  tweets.\n",
    "We use an existing model \"en_core_web_md\"( English medium sized model).\n",
    "This model by deafult has POS tagger, Dependency parser and Named entity recognition functionalities\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "We only re-train the text categorization  part of the model.\n",
    "<br>\n",
    "We have the following model architecture  available for training:\n",
    "\n",
    "<br>\n",
    "* \"bow\"\tAn ngram “bag-of-words” model. This architecture should run much faster than the others, but may not be as accurate, especially if texts are short. The features extracted can be controlled using the keyword arguments ngram_size and attr. For instance, ngram_size=3 and attr=\"lower\" would give lower-cased unigram, trigram and bigram features. 2, 3 or 4 are usually good choices of ngram size.\n",
    "<br>\n",
    "* \"simple_cnn\"\tA neural network model where token vectors are calculated using a CNN. The vectors are mean pooled and used as features in a feed-forward network. This architecture is usually less accurate than the ensemble, but runs faster.\n",
    "\n",
    "<br>\n",
    "* \"ensemble\"\tDefault: Stacked ensemble of a bag-of-words model and a neural network model. The neural network uses a CNN with mean pooling and attention. The “ngram_size” and “attr” arguments can be used to configure the feature extraction for the bag-of-words model.\n",
    "\n",
    "***\n",
    "Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. ... The term dropout refers to randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network.\n",
    "In our case if dropout = 0.5 there is a 50% dropping out otmitting units during training process of our model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_spacy(  train_data, iterations,test_texts,test_cats, model_arch, dropout = 0.3, model=None,init_tok2vec=None):\n",
    "    ''' Train a spacy NER model, which can be queried against with test data\n",
    "   \n",
    "    train_data : training data in the format of (sentence, {cats: ['positive'|'negative'|'neutral']})\n",
    "    labels : a list of unique annotations\n",
    "    iterations : number of training iterations\n",
    "    dropout : dropout proportion for training\n",
    "    display_freq : number of epochs between logging losses to console\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": model_arch}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label(\"positive\")\n",
    "    textcat.add_label(\"negative\")\n",
    "    textcat.add_label(\"neutral\")\n",
    "\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        if init_tok2vec is not None:\n",
    "            with init_tok2vec.open(\"rb\") as file_:\n",
    "                textcat.model.tok2vec.from_bytes(file_.read())\n",
    "        print(\"Training the model...\")\n",
    "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "        batch_sizes = compounding(16.0, 64.0, 1.5)\n",
    "        for i in range(iterations):\n",
    "            print('Iteration: '+str(i))\n",
    "            start_time = time.clock()\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=dropout, losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the test data \n",
    "                evaluate(nlp.tokenizer, textcat, test_texts,test_cats)\n",
    "            print ('Elapsed time'+str(time.clock() - start_time)+  \"seconds\")\n",
    "        with nlp.use_params(optimizer.averages):\n",
    "            modelName = model_arch+\"TweetClassification\"\n",
    "            filepath = \"C:\\\\TweetSenitment\\\\\"+modelName+\"\\\\\"\n",
    "            nlp.to_disk(filepath)\n",
    "    return nlp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "Iteration: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.60      0.68      1075\n",
      "    negative       0.72      0.39      0.51       983\n",
      "     neutral       0.54      0.81      0.65      1376\n",
      "\n",
      "    accuracy                           0.63      3434\n",
      "   macro avg       0.68      0.60      0.61      3434\n",
      "weighted avg       0.67      0.63      0.62      3434\n",
      "\n",
      "Elapsed time17.34271690000014seconds\n",
      "Iteration: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.63      0.70      1075\n",
      "    negative       0.73      0.47      0.58       983\n",
      "     neutral       0.57      0.80      0.66      1376\n",
      "\n",
      "    accuracy                           0.65      3434\n",
      "   macro avg       0.69      0.63      0.64      3434\n",
      "weighted avg       0.68      0.65      0.65      3434\n",
      "\n",
      "Elapsed time16.7559954999997seconds\n",
      "Iteration: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.64      0.70      1075\n",
      "    negative       0.70      0.51      0.59       983\n",
      "     neutral       0.57      0.76      0.65      1376\n",
      "\n",
      "    accuracy                           0.65      3434\n",
      "   macro avg       0.68      0.64      0.65      3434\n",
      "weighted avg       0.67      0.65      0.65      3434\n",
      "\n",
      "Elapsed time16.98981380000032seconds\n",
      "Iteration: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.65      0.71      1075\n",
      "    negative       0.70      0.52      0.60       983\n",
      "     neutral       0.58      0.75      0.65      1376\n",
      "\n",
      "    accuracy                           0.65      3434\n",
      "   macro avg       0.68      0.64      0.65      3434\n",
      "weighted avg       0.67      0.65      0.65      3434\n",
      "\n",
      "Elapsed time17.19565680000005seconds\n",
      "Iteration: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.66      0.71      1075\n",
      "    negative       0.70      0.53      0.61       983\n",
      "     neutral       0.58      0.75      0.66      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.69      0.65      0.66      3434\n",
      "weighted avg       0.68      0.66      0.66      3434\n",
      "\n",
      "Elapsed time16.91032350000023seconds\n",
      "Iteration: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.67      0.72      1075\n",
      "    negative       0.69      0.54      0.61       983\n",
      "     neutral       0.59      0.74      0.65      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.69      0.65      0.66      3434\n",
      "weighted avg       0.68      0.66      0.66      3434\n",
      "\n",
      "Elapsed time17.099888799999917seconds\n",
      "Iteration: 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.67      0.72      1075\n",
      "    negative       0.68      0.55      0.61       983\n",
      "     neutral       0.59      0.73      0.65      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.68      0.65      0.66      3434\n",
      "weighted avg       0.67      0.66      0.66      3434\n",
      "\n",
      "Elapsed time16.958836699999665seconds\n",
      "Iteration: 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.67      0.72      1075\n",
      "    negative       0.68      0.56      0.61       983\n",
      "     neutral       0.59      0.73      0.65      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.68      0.65      0.66      3434\n",
      "weighted avg       0.67      0.66      0.66      3434\n",
      "\n",
      "Elapsed time16.326557600000342seconds\n",
      "Iteration: 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.67      0.72      1075\n",
      "    negative       0.68      0.57      0.62       983\n",
      "     neutral       0.59      0.72      0.65      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.68      0.65      0.66      3434\n",
      "weighted avg       0.67      0.66      0.66      3434\n",
      "\n",
      "Elapsed time16.809067900000173seconds\n",
      "Iteration: 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.67      0.72      1075\n",
      "    negative       0.67      0.57      0.62       983\n",
      "     neutral       0.60      0.72      0.65      1376\n",
      "\n",
      "    accuracy                           0.66      3434\n",
      "   macro avg       0.68      0.66      0.66      3434\n",
      "weighted avg       0.67      0.66      0.66      3434\n",
      "\n",
      "Elapsed time16.64938710000024seconds\n"
     ]
    }
   ],
   "source": [
    "# Train (and save) the Text categorization model with BOW\n",
    "nlp = train_spacy(training_data, 10,test_texts,test_cats,\"bow\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "Iteration: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.73      0.75      1075\n",
      "    negative       0.70      0.66      0.68       983\n",
      "     neutral       0.65      0.71      0.68      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.71      0.70      0.70      3434\n",
      "weighted avg       0.71      0.70      0.70      3434\n",
      "\n",
      "Elapsed time50.510161600000174seconds\n",
      "Iteration: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.75      0.76      1075\n",
      "    negative       0.71      0.68      0.70       983\n",
      "     neutral       0.67      0.69      0.68      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time35.800802300000214seconds\n",
      "Iteration: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.75      0.76      1075\n",
      "    negative       0.70      0.69      0.70       983\n",
      "     neutral       0.67      0.69      0.68      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.72      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time32.848710199999914seconds\n",
      "Iteration: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.75      0.76      1075\n",
      "    negative       0.70      0.70      0.70       983\n",
      "     neutral       0.67      0.68      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time34.912241600000016seconds\n",
      "Iteration: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.76      0.76      1075\n",
      "    negative       0.69      0.70      0.70       983\n",
      "     neutral       0.67      0.67      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time38.746044399999846seconds\n",
      "Iteration: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.77      0.76      0.76      1075\n",
      "    negative       0.69      0.71      0.70       983\n",
      "     neutral       0.68      0.67      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time39.78450840000005seconds\n",
      "Iteration: 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.75      0.76      1075\n",
      "    negative       0.69      0.70      0.70       983\n",
      "     neutral       0.67      0.67      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time38.658199700000296seconds\n",
      "Iteration: 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.75      0.76      1075\n",
      "    negative       0.69      0.70      0.70       983\n",
      "     neutral       0.68      0.67      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time39.10940750000009seconds\n",
      "Iteration: 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.75      0.75      1075\n",
      "    negative       0.68      0.70      0.69       983\n",
      "     neutral       0.67      0.66      0.67      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n",
      "Elapsed time38.901954699999806seconds\n",
      "Iteration: 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.76      0.76      1075\n",
      "    negative       0.69      0.69      0.69       983\n",
      "     neutral       0.67      0.66      0.66      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n",
      "Elapsed time38.738234500000544seconds\n"
     ]
    }
   ],
   "source": [
    "# Train (and save) the Text categorization model with Simple CNN\n",
    "nlp = train_spacy(training_data, 10,test_texts,test_cats,\"simple_cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "Iteration: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.76      0.75      1075\n",
      "    negative       0.70      0.66      0.68       983\n",
      "     neutral       0.67      0.69      0.68      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.71      0.70      0.71      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n",
      "Elapsed time48.21980220000023seconds\n",
      "Iteration: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.78      0.77      1075\n",
      "    negative       0.69      0.69      0.69       983\n",
      "     neutral       0.69      0.67      0.68      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time52.30921480000006seconds\n",
      "Iteration: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.79      0.77      1075\n",
      "    negative       0.69      0.71      0.70       983\n",
      "     neutral       0.70      0.66      0.68      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.72      0.72      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time56.234135199999855seconds\n",
      "Iteration: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.79      0.77      1075\n",
      "    negative       0.69      0.71      0.70       983\n",
      "     neutral       0.69      0.66      0.68      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.72      0.72      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time66.23226280000017seconds\n",
      "Iteration: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.77      0.77      1075\n",
      "    negative       0.69      0.70      0.70       983\n",
      "     neutral       0.69      0.66      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time79.96554349999997seconds\n",
      "Iteration: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.77      0.76      1075\n",
      "    negative       0.69      0.71      0.70       983\n",
      "     neutral       0.68      0.66      0.67      1376\n",
      "\n",
      "    accuracy                           0.71      3434\n",
      "   macro avg       0.71      0.71      0.71      3434\n",
      "weighted avg       0.71      0.71      0.71      3434\n",
      "\n",
      "Elapsed time80.71131819999937seconds\n",
      "Iteration: 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.76      0.76      1075\n",
      "    negative       0.68      0.70      0.69       983\n",
      "     neutral       0.68      0.65      0.66      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n",
      "Elapsed time80.68020449999949seconds\n",
      "Iteration: 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.76      0.75      1075\n",
      "    negative       0.68      0.70      0.69       983\n",
      "     neutral       0.67      0.65      0.66      1376\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n",
      "Elapsed time81.33570920000057seconds\n",
      "Iteration: 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.76      0.75      1075\n",
      "    negative       0.68      0.69      0.68       983\n",
      "     neutral       0.66      0.65      0.66      1376\n",
      "\n",
      "    accuracy                           0.69      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.69      0.69      0.69      3434\n",
      "\n",
      "Elapsed time81.6167129000005seconds\n",
      "Iteration: 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.76      0.75      1075\n",
      "    negative       0.68      0.69      0.69       983\n",
      "     neutral       0.66      0.64      0.65      1376\n",
      "\n",
      "    accuracy                           0.69      3434\n",
      "   macro avg       0.70      0.70      0.70      3434\n",
      "weighted avg       0.69      0.69      0.69      3434\n",
      "\n",
      "Elapsed time80.78477310000017seconds\n"
     ]
    }
   ],
   "source": [
    "# Train (and save) the Text categorization model with ensemble\n",
    "nlp = train_spacy(training_data, 10,test_texts,test_cats,\"ensmeble\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our model on  test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: want david cook\n",
      "Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.21973687410354614, 'negative': 0.22979359328746796, 'neutral': 0.5504695177078247}\n",
      "=======================================\n",
      "Text: okaii cool cant wait series begin guna awesome\n",
      " Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.9604972004890442, 'negative': 0.0052943420596420765, 'neutral': 0.03420846536755562}\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"C:\\\\TweetSenitment\\\\bowTweetClassification\\\\\")\n",
    "doc2 = nlp2(test_texts[100])\n",
    "print(\"Text: \"+ test_texts[100])\n",
    "print(\"Orig Cat:\"+ test_cats[100])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)\n",
    "print(\"=======================================\")\n",
    "doc2 = nlp2(test_texts[1000])\n",
    "print(\"Text: \"+ test_texts[1000])\n",
    "print(\" Orig Cat:\"+test_cats[1000])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: want david cook\n",
      "Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.007848287932574749, 'negative': 0.012497424148023129, 'neutral': 0.9796542525291443}\n",
      "=======================================\n",
      "Text: okaii cool cant wait series begin guna awesome\n",
      " Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.9424135684967041, 'negative': 0.002971380716189742, 'neutral': 0.05461500957608223}\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"C:\\\\TweetSenitment\\\\simple_cnnTweetClassification\\\\\")\n",
    "doc2 = nlp2(test_texts[100])\n",
    "print(\"Text: \"+ test_texts[100])\n",
    "print(\"Orig Cat:\"+ test_cats[100])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)\n",
    "print(\"=======================================\")\n",
    "doc2 = nlp2(test_texts[1000])\n",
    "print(\"Text: \"+ test_texts[1000])\n",
    "print(\" Orig Cat:\"+test_cats[1000])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: want david cook\n",
      "Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.03677338361740112, 'negative': 0.01559833437204361, 'neutral': 0.9476282596588135}\n",
      "=======================================\n",
      "Text: okaii cool cant wait series begin guna awesome\n",
      " Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.9470809698104858, 'negative': 0.005646079778671265, 'neutral': 0.04727298766374588}\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"C:\\\\TweetSenitment\\\\ensmebleTweetClassification\\\\\")\n",
    "doc2 = nlp2(test_texts[100])\n",
    "print(\"Text: \"+ test_texts[100])\n",
    "print(\"Orig Cat:\"+ test_cats[100])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)\n",
    "print(\"=======================================\")\n",
    "doc2 = nlp2(test_texts[1000])\n",
    "print(\"Text: \"+ test_texts[1000])\n",
    "print(\" Orig Cat:\"+test_cats[1000])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
